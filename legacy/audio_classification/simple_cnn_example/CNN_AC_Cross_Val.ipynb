{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Classification with CNN under Cross-Validation Experiment\n",
    "Try to analysis the data with,\n",
    "+ accuracy\n",
    "+ precision\n",
    "+ recall\n",
    "  \n",
    "both of training step and validation stage.\n",
    "Meanwhile, try to use the plot-box to demonstrates the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cross-Validation under Multi-Classification\n",
    "`This explaination comes from the Chat-GPT4o.`\n",
    "Calculating precision and recall for a multi-class classification algorithm involves considering each class separately and then aggregating the results. Here's how you can do it step by step:\n",
    "\n",
    "### 1. Definitions\n",
    "\n",
    "- **Precision** for a class $C_i$: The ratio of true positive predictions for class $C_i$ to the total number of instances predicted as $C_i$ (i.e., true positives + false positives).\n",
    "  \n",
    "  $$\n",
    "  \\text{Precision}(C_i) = \\frac{\\text{True Positives}(C_i)}{\\text{True Positives}(C_i) + \\text{False Positives}(C_i)}\n",
    "  $$\n",
    "\n",
    "- **Recall** for a class $C_i $: The ratio of true positive predictions for class $ C_i $ to the total number of instances that actually belong to $ C_i $ (i.e., true positives + false negatives).\n",
    "\n",
    "  $$\n",
    "  \\text{Recall}(C_i) = \\frac{\\text{True Positives}(C_i)}{\\text{True Positives}(C_i) + \\text{False Negatives}(C_i)}\n",
    "  $$\n",
    "\n",
    "### 2. Confusion Matrix\n",
    "\n",
    "For a multi-class classification problem, you typically generate a confusion matrix, where each row represents the actual class, and each column represents the predicted class. The element at position $ (i, j) $ in the confusion matrix represents the number of instances of class $ i $ that were predicted as class $ j $.\n",
    "\n",
    "### 3. Calculation for Each Class\n",
    "\n",
    "- **True Positives (TP)**: The diagonal element for the class $ C_i $ in the confusion matrix, i.e., $ \\text{TP}(C_i) = \\text{Confusion Matrix}[i][i] $.\n",
    "- **False Positives (FP)**: The sum of the corresponding column $ i $ minus the true positives, i.e., $ \\text{FP}(C_i) = \\sum_j \\text{Confusion Matrix}[j][i] - \\text{Confusion Matrix}[i][i] $.\n",
    "- **False Negatives (FN)**: The sum of the corresponding row $ i $ minus the true positives, i.e., $ \\text{FN}(C_i) = \\sum_j \\text{Confusion Matrix}[i][j] - \\text{Confusion Matrix}[i][i] $.\n",
    "\n",
    "### 4. Precision and Recall for Each Class\n",
    "\n",
    "Using the definitions and the components from the confusion matrix, you can calculate precision and recall for each class $ C_i $:\n",
    "\n",
    "$$\n",
    "\\text{Precision}(C_i) = \\frac{\\text{TP}(C_i)}{\\text{TP}(C_i) + \\text{FP}(C_i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Recall}(C_i) = \\frac{\\text{TP}(C_i)}{\\text{TP}(C_i) + \\text{FN}(C_i)}\n",
    "$$\n",
    "\n",
    "### 5. Aggregating Results\n",
    "\n",
    "To get overall metrics, you can use macro-averaging or micro-averaging:\n",
    "\n",
    "- **Macro-Averaging**: Calculate the metrics for each class independently and then take the average.\n",
    "\n",
    "  $$\n",
    "  \\text{Macro Precision} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{Precision}(C_i)\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\text{Macro Recall} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{Recall}(C_i)\n",
    "  $$\n",
    "\n",
    "- **Micro-Averaging**: Aggregate the contributions of all classes to calculate the metrics. Essentially, you sum up the true positives, false positives, and false negatives across all classes and then calculate precision and recall.\n",
    "\n",
    "  $$\n",
    "  \\text{Micro Precision} = \\frac{\\sum_{i=1}^{N} \\text{TP}(C_i)}{\\sum_{i=1}^{N} (\\text{TP}(C_i) + \\text{FP}(C_i))}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\text{Micro Recall} = \\frac{\\sum_{i=1}^{N} \\text{TP}(C_i)}{\\sum_{i=1}^{N} (\\text{TP}(C_i) + \\text{FN}(C_i))}\n",
    "  $$\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's consider a simple example with three classes A, B, and C, and the following confusion matrix:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|ccc}\n",
    "    & \\text{Pred A} & \\text{Pred B} & \\text{Pred C} \\\\\n",
    "    \\hline\n",
    "    \\text{Actual A} & 40 & 5 & 5 \\\\\n",
    "    \\text{Actual B} & 3 & 50 & 2 \\\\\n",
    "    \\text{Actual C} & 2 & 4 & 45 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "For class A:\n",
    "\n",
    "- $\\text{TP}(A) = 40$\n",
    "- $\\text{FP}(A) = 3 + 2 = 5$\n",
    "- $\\text{FN}(A) = 5 + 5 = 10$\n",
    "\n",
    "$$\n",
    "\\text{Precision}(A) = \\frac{40}{40 + 5} = \\frac{40}{45} \\approx 0.89\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Recall}(A) = \\frac{40}{40 + 10} = \\frac{40}{50} = 0.8\n",
    "$$\n",
    "\n",
    "You can calculate precision and recall similarly for classes B and C and then aggregate the results using macro-averaging or micro-averaging as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class  \n",
       "0          dog_bark  \n",
       "1  children_playing  \n",
       "2  children_playing  \n",
       "3  children_playing  \n",
       "4  children_playing  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path \n",
    "\n",
    "datasource_path = Path.home()/'dataset'/'UrbanSound8K'\n",
    "\n",
    "# read the metadata file\n",
    "metadata_file_path = datasource_path/'metadata'/'UrbanSound8K.csv'\n",
    "df = pd.read_csv(metadata_file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>/fold5/100032-3-0-0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>/fold5/100263-2-0-117.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>/fold5/100263-2-0-121.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>/fold5/100263-2-0-126.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>/fold5/100263-2-0-137.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class                       path  \n",
       "0          dog_bark    /fold5/100032-3-0-0.wav  \n",
       "1  children_playing  /fold5/100263-2-0-117.wav  \n",
       "2  children_playing  /fold5/100263-2-0-121.wav  \n",
       "3  children_playing  /fold5/100263-2-0-126.wav  \n",
       "4  children_playing  /fold5/100263-2-0-137.wav  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['path'] = '/fold' + df['fold'].astype(str) + '/' + df['slice_file_name'].astype(str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "libtorch_cuda.so: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27404/1550227992.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrossValUtils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcalIndexes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwavDataUtil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWavDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0maudio_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasource_path\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'audio'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maudioDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWavDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Test-time-Adaptation-in-AC/legacy/audio_classification/simple_cnn_example/lib/wavDataUtil.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwavUtil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWavOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/torchaudio/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from torchaudio import (  # noqa: F401\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0m_extension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcompliance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfunctional\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/torchaudio/_extension/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0m_IS_KALDI_AVAILABLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_IS_TORCHAUDIO_EXT_AVAILABLE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"libtorchaudio\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torchaudio\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/torchaudio/_extension/utils.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/torch/_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;31m# static (global) initialization code in order to register custom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;31m# operators with the JIT.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_libraries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: libtorch_cuda.so: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "from lib.crossValUtils import calIndexes\n",
    "from lib.wavDataUtil import WavDataset\n",
    "audio_path = datasource_path/'audio'\n",
    "\n",
    "audioDS = WavDataset(df= df, data_path= audio_path)\n",
    "\n",
    "audio, class_id = audioDS[0]\n",
    "audio.shape, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.crossValUtils import calIndexes, switchFold\n",
    "\n",
    "indexes = calIndexes(dataset=audioDS, n_flod=10)\n",
    "train_ids, val_ids = switchFold(val_fold=2, indexes=indexes)\n",
    "\n",
    "# train_ids = df[df['fold'] != 5].index.to_numpy()\n",
    "# val_ids = df[df['fold'] == 5].index.to_numpy()\n",
    "\n",
    "train_ids.shape, val_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.crossValUtils import SubsetDs\n",
    "\n",
    "train_ds = SubsetDs(dataset=audioDS, indexes=train_ids)\n",
    "val_ds = SubsetDs(dataset=audioDS, indexes=val_ids)\n",
    "\n",
    "audio, class_id = val_ds[9]\n",
    "audio.shape, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = DataLoader(dataset=val_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "feature, labels = next(iter(val_dl))\n",
    "feature.shape, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from lib.acModel import AudioClassifier\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = AudioClassifier().to(device=device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.acModel import Processor\n",
    "Processor.training(model=model, train_dl=train_dl, num_epochs=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.crossValUtils import ValidationRecord\n",
    "import torch\n",
    "\n",
    "vr = ValidationRecord(n_fold=10, label_size=10)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for (feature, labels) in val_dl:\n",
    "        feature, labels = feature.to(device), labels.to(device)\n",
    "        outputs = model(feature)\n",
    "        vr.noteRecord(outputs=outputs, labels=labels, val_fold=2, iter=0)\n",
    "\n",
    "vr.calRecord()\n",
    "records = vr.getRecord()\n",
    "records[records['val_fold'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records[records['val_fold']==5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processor.inference(model=model, val_dl=val_dl, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.crossValUtils import corss_validation_analysis\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def training(dataset: Dataset, indexes):\n",
    "    train_ds = SubsetDs(dataset=dataset, indexes=indexes)\n",
    "    train_dl = DataLoader(dataset=train_ds, batch_size=16, shuffle=True)\n",
    "    model = AudioClassifier().to(device=device)\n",
    "    Processor.training(model=model, train_dl=train_dl, num_epochs=1, device=device)\n",
    "    return model\n",
    "\n",
    "def inference(model, iter: int, val_fold: int, dataset: Dataset, indexes, records: ValidationRecord):\n",
    "    val_ds = SubsetDs(dataset=dataset, indexes=indexes)\n",
    "    val_dl = DataLoader(dataset=val_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (feature, labels) in val_dl:\n",
    "            feature, labels = feature.to(device), labels.to(device)\n",
    "            outputs = model(feature)\n",
    "            records.noteRecord(outputs=outputs, labels=labels, val_fold=val_fold, iter=iter)\n",
    "\n",
    "rc = corss_validation_analysis(dataset=audioDS, n_fold=10, label_size=10, tranFunc=training, inferFunc=inference, n_iter=2)\n",
    "rc.calRecord()\n",
    "rc_df = rc.getRecord()\n",
    "rc_df.to_csv('CNN_AC_Cross_Val.csv', sep=',', header=True, encoding='utf-8', mode='w')\n",
    "vLog = rc.getValidateLog()\n",
    "vLog.to_csv('CNN_AC_Cross_Val_Log.csv', sep=',', header=True, encoding='utf-8', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = torch.ones((20,1))\n",
    "iters * 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
