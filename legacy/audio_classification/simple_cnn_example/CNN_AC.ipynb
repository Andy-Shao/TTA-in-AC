{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Deep Learning Made Simple: Sound Classification, Step-by-Step\n",
    "## Dataset Usage Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path \n",
    "\n",
    "datasource_path = Path.home()/'dataset'/'UrbanSound8K'\n",
    "\n",
    "# read the metadata file\n",
    "metadata_file_path = datasource_path/'metadata'/'UrbanSound8K.csv'\n",
    "df = pd.read_csv(metadata_file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['path'] = '/fold' + df['fold'].astype(str) + '/' + df['slice_file_name'].astype(str)\n",
    "df = df[['path', 'classID']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio File Reading\n",
    "```python\n",
    "pip install playsound\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playsound import playsound\n",
    "audio_path = datasource_path/'audio'\n",
    "audio_sample_path = audio_path/'fold5'/'100032-3-0-0.wav'\n",
    "print(audio_sample_path)\n",
    "playsound(str(audio_sample_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.wavUtil import WavOps\n",
    "(sig, sr) = WavOps.open(audio_sample_path)\n",
    "sig.shape, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(sig.numpy().T)\n",
    "plt.title('Audio Waveform')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.xlabel('Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = WavOps.resampleRate(audio=(sig, sr), new_sample_rate=44100)\n",
    "audio = WavOps.rechannel(audio=audio, channel_num=2)\n",
    "audio = WavOps.pad_trunc(audio=audio, max_ms=4000)\n",
    "audio = WavOps.time_shift(audio=audio, shift_limit=.4)\n",
    "audio = WavOps.spectro_gram(audio=audio, n_mels=64, n_fft=1024, hop_len=None)\n",
    "audio = WavOps.spectro_augment(spec=audio, max_mask_perctage=.1, freq_mask_num=2, time_mask_num=2)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.imshow(audio[0].detach().numpy(), cmap='viridis', origin='lower', aspect='auto')\n",
    "plt.title('Mel Spectrogram in channel 0')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.imshow(audio[1].detach().numpy(), cmap='viridis', origin='lower', aspect='auto')\n",
    "plt.title('Mel Spectrogram in channel 1')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from lib.wavDataUtil import WavDataset\n",
    "\n",
    "audioDS = WavDataset(df, audio_path)\n",
    "\n",
    "audio, class_id = audioDS[0]\n",
    "audio.shape, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = len(audioDS)\n",
    "train_num = round(sample_num * .8)\n",
    "val_num = sample_num - train_num\n",
    "\n",
    "train_ds, val_ds = random_split(audioDS, [train_num, val_num])\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = DataLoader(dataset=val_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "feature, label = next(iter(train_dl))\n",
    "feature.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.imshow(audio[0].detach().numpy(), cmap='viridis', origin='lower', aspect='auto')\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from lib.acModel import AudioClassifier\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = AudioClassifier().to(device=device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.acModel import Processor\n",
    "\n",
    "Processor.training(model=model, train_dl=train_dl, device=device, num_epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
