{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Commands Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64721, 51088, 6835, 6798)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from lib.scDataset import SpeechCommandsDataset\n",
    "\n",
    "dataset_path = '/root/data/speech_commands'\n",
    "full_dataset = SpeechCommandsDataset(root_path=dataset_path, mode='full', include_rate=True)\n",
    "train_dataset = SpeechCommandsDataset(root_path=dataset_path, mode='train', include_rate=False)\n",
    "test_dataset = SpeechCommandsDataset(root_path=dataset_path, mode='test', include_rate=False)\n",
    "val_dataset = SpeechCommandsDataset(root_path=dataset_path, mode='validation', include_rate=False)\n",
    "\n",
    "len(full_dataset), len(train_dataset), len(test_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({16000}, 30)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rates = set()\n",
    "labels = set()\n",
    "audio_shape = set()\n",
    "for audio, label, sample_rate in full_dataset:\n",
    "    sample_rates.add(sample_rate)\n",
    "    labels.add(label)\n",
    "    audio_shape.add(audio.shape)\n",
    "\n",
    "sample_rates, len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{torch.Size([1, 5945]),\n",
       " torch.Size([1, 6144]),\n",
       " torch.Size([1, 6688]),\n",
       " torch.Size([1, 6827]),\n",
       " torch.Size([1, 7339]),\n",
       " torch.Size([1, 7431]),\n",
       " torch.Size([1, 7510]),\n",
       " torch.Size([1, 7851]),\n",
       " torch.Size([1, 8022]),\n",
       " torch.Size([1, 8174]),\n",
       " torch.Size([1, 8192]),\n",
       " torch.Size([1, 8363]),\n",
       " torch.Size([1, 8534]),\n",
       " torch.Size([1, 8875]),\n",
       " torch.Size([1, 8917]),\n",
       " torch.Size([1, 9046]),\n",
       " torch.Size([1, 9387]),\n",
       " torch.Size([1, 9558]),\n",
       " torch.Size([1, 9660]),\n",
       " torch.Size([1, 9728]),\n",
       " torch.Size([1, 9899]),\n",
       " torch.Size([1, 10032]),\n",
       " torch.Size([1, 10070]),\n",
       " torch.Size([1, 10240]),\n",
       " torch.Size([1, 10403]),\n",
       " torch.Size([1, 10411]),\n",
       " torch.Size([1, 10582]),\n",
       " torch.Size([1, 10752]),\n",
       " torch.Size([1, 10923]),\n",
       " torch.Size([1, 11094]),\n",
       " torch.Size([1, 11146]),\n",
       " torch.Size([1, 11264]),\n",
       " torch.Size([1, 11435]),\n",
       " torch.Size([1, 11520]),\n",
       " torch.Size([1, 11606]),\n",
       " torch.Size([1, 11776]),\n",
       " torch.Size([1, 11889]),\n",
       " torch.Size([1, 11947]),\n",
       " torch.Size([1, 12118]),\n",
       " torch.Size([1, 12261]),\n",
       " torch.Size([1, 12288]),\n",
       " torch.Size([1, 12459]),\n",
       " torch.Size([1, 12480]),\n",
       " torch.Size([1, 12630]),\n",
       " torch.Size([1, 12632]),\n",
       " torch.Size([1, 12800]),\n",
       " torch.Size([1, 12971]),\n",
       " torch.Size([1, 13004]),\n",
       " torch.Size([1, 13142]),\n",
       " torch.Size([1, 13312]),\n",
       " torch.Size([1, 13375]),\n",
       " torch.Size([1, 13483]),\n",
       " torch.Size([1, 13654]),\n",
       " torch.Size([1, 13824]),\n",
       " torch.Size([1, 13995]),\n",
       " torch.Size([1, 14118]),\n",
       " torch.Size([1, 14166]),\n",
       " torch.Size([1, 14336]),\n",
       " torch.Size([1, 14490]),\n",
       " torch.Size([1, 14507]),\n",
       " torch.Size([1, 14678]),\n",
       " torch.Size([1, 14848]),\n",
       " torch.Size([1, 14861]),\n",
       " torch.Size([1, 15019]),\n",
       " torch.Size([1, 15153]),\n",
       " torch.Size([1, 15190]),\n",
       " torch.Size([1, 15233]),\n",
       " torch.Size([1, 15360]),\n",
       " torch.Size([1, 15473]),\n",
       " torch.Size([1, 15531]),\n",
       " torch.Size([1, 15604]),\n",
       " torch.Size([1, 15702]),\n",
       " torch.Size([1, 15793]),\n",
       " torch.Size([1, 15872]),\n",
       " torch.Size([1, 15976]),\n",
       " torch.Size([1, 16000])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pink_noise, torch.Size([1, 960000]), 16000\n",
      "running_tap, torch.Size([1, 978488]), 16000\n",
      "exercise_bike, torch.Size([1, 980062]), 16000\n",
      "doing_the_dishes, torch.Size([1, 1522930]), 16000\n",
      "dude_miaowing, torch.Size([1, 988891]), 16000\n",
      "white_noise, torch.Size([1, 960000]), 16000\n"
     ]
    }
   ],
   "source": [
    "from lib.scDataset import BackgroundNoise\n",
    "noise_dataset = BackgroundNoise(root_path=dataset_path)\n",
    "for noise_type, noise, sample_rate in noise_dataset:\n",
    "    print(f'{noise_type}, {noise.shape}, {sample_rate}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
